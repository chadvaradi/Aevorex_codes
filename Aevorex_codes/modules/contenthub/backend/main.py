"""
==========================================================================
CONTENTHUB BACKEND API v2.0 - OPENROUTER INTEGR√ÅLT
==========================================================================

Pr√©mium FastAPI backend multimod√°lis tartalom gener√°l√°shoz √©s prompt engineeringhez.
Teljes OpenRouter integr√°ci√≥ k√∂lts√©ghat√©kony AI modellekkel.

F≈ëbb funkci√≥k:
- Prompt Studio API (MidJourney, DALL-E, Runway optimaliz√°l√°s)
- Caption Master (Instagram, LinkedIn, TikTok captionok)
- Visual Prompter (multimod√°lis k√©ple√≠r√°s √©s prompt gener√°l√°s)
- Audio Scripter (podcast, voiceover, zenei briefek)
- Brand Templater (brand konzisztencia ellen≈ërz√©s)
- Workflow Manager (komplex multi-step tartalom pipeline)

Technol√≥gi√°k:
- FastAPI + Pydantic valid√°ci√≥
- OpenRouter API integr√°ci√≥ (ingyenes/olcs√≥ modellek)
- asyncio aszinkron feldolgoz√°s
- SQLAlchemy adatb√°zis ORM
- K√∂lts√©ghat√©kony caching rendszer

Szerz≈ëk: Aevorex Premium Team
Utols√≥ friss√≠t√©s: 2025-01-XX
"""

import asyncio
import logging
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union
from pathlib import Path
import os
from contextlib import asynccontextmanager

# FastAPI √©s kapcsol√≥d√≥ importok
try:
    from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends
    from fastapi.middleware.cors import CORSMiddleware
    from fastapi.middleware.gzip import GZipMiddleware
    from fastapi.responses import JSONResponse
    from pydantic import BaseModel, Field, validator, field_validator
    FASTAPI_AVAILABLE = True
except ImportError:
    FASTAPI_AVAILABLE = False
    print("‚ö†Ô∏è FastAPI not installed. Install with: pip install fastapi uvicorn")

# Adatb√°zis kapcsolatok
try:
    from sqlalchemy import Column, Integer, String, Text, JSON, DateTime, Boolean, create_engine
    from sqlalchemy.ext.declarative import declarative_base
    from sqlalchemy.orm import sessionmaker, Session
    SQLALCHEMY_AVAILABLE = True
except ImportError:
    SQLALCHEMY_AVAILABLE = False
    print("‚ö†Ô∏è SQLAlchemy not installed. Install with: pip install sqlalchemy")

# HTTP kliens √©s AI szolg√°ltat√°sok
try:
    import httpx
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("‚ö†Ô∏è OpenAI client not installed. Install with: pip install openai httpx")

# ==========================================================================
# CONFIGURATION & ENVIRONMENT
# ==========================================================================

# Configuration from environment
OPENROUTER_API_KEY = os.getenv("CONTENTHUB_OPENROUTER_API_KEY", "")
PRIMARY_MODEL = os.getenv("CONTENTHUB_PRIMARY_MODEL", "deepseek/deepseek-r1:free")
FALLBACK_MODEL = os.getenv("CONTENTHUB_FALLBACK_MODEL", "meta-llama/llama-3.3-8b-instruct:free")
MULTIMODAL_MODEL = os.getenv("CONTENTHUB_MULTIMODAL_MODEL", "google/gemini-flash-1.5")
PREMIUM_MODEL = os.getenv("CONTENTHUB_PREMIUM_MODEL", "gpt-4o-mini")
TEMPERATURE = float(os.getenv("CONTENTHUB_TEMPERATURE", "0.7"))
MAX_TOKENS = int(os.getenv("CONTENTHUB_MAX_TOKENS", "2048"))
API_TIMEOUT = int(os.getenv("CONTENTHUB_API_TIMEOUT", "30"))
API_RETRIES = int(os.getenv("CONTENTHUB_API_RETRIES", "3"))
CACHE_DURATION = int(os.getenv("CONTENTHUB_CACHE_DURATION", "3600"))
RATE_LIMIT_CALLS = int(os.getenv("CONTENTHUB_RATE_LIMIT_CALLS", "100"))
RATE_LIMIT_WINDOW = int(os.getenv("CONTENTHUB_RATE_LIMIT_WINDOW", "60"))

# Optional database configuration
try:
    DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///contenthub.db")
except Exception:
    DATABASE_URL = "sqlite:///contenthub.db"

# OpenRouter API konfigur√°ci√≥ environment v√°ltoz√≥kb√≥l
OPENROUTER_BASE_URL = os.getenv("CONTENTHUB_AI__OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
SITE_URL = os.getenv("CONTENTHUB_AI__SITE_URL", "http://localhost:3002")
APP_NAME = os.getenv("CONTENTHUB_AI__APP_NAME", "Aevorex ContentHUB")

# AI modellek konfigur√°ci√≥ja
MODEL_PRIMARY = PRIMARY_MODEL
MODEL_FALLBACK = FALLBACK_MODEL
MODEL_MULTIMODAL = MULTIMODAL_MODEL
MODEL_PREMIUM = PREMIUM_MODEL

# AI param√©terek
DEFAULT_TEMPERATURE = TEMPERATURE
DEFAULT_MAX_TOKENS = MAX_TOKENS
DEFAULT_TIMEOUT = API_TIMEOUT
RETRY_COUNT = API_RETRIES

# Cache be√°ll√≠t√°sok
CACHE_ENABLED = True

# Rate limiting
RATE_LIMIT_RPM = RATE_LIMIT_CALLS
DAILY_TOKEN_LIMIT = RATE_LIMIT_CALLS

# Logging configuration
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ==========================================================================
# DATABASE MODELS
# ==========================================================================

if SQLALCHEMY_AVAILABLE:
    Base = declarative_base()

    class ContentGeneration(Base):
        __tablename__ = "content_generations"
        
        id = Column(Integer, primary_key=True, index=True)
        module_type = Column(String(50), nullable=False)  # prompt-studio, caption-master, etc.
        user_session = Column(String(100), nullable=True)
        input_data = Column(JSON, nullable=False)
        generated_content = Column(Text, nullable=True)
        metadata = Column(JSON, nullable=True)
        status = Column(String(20), default="pending")  # pending, processing, completed, failed
        created_at = Column(DateTime, default=datetime.utcnow)
        updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
        generation_time_ms = Column(Integer, nullable=True)
        rating = Column(Integer, nullable=True)  # 1-5 user rating
        ai_model_used = Column(String(100), nullable=True)
        token_usage = Column(JSON, nullable=True)  # input/output token counts

    class WorkflowExecution(Base):
        __tablename__ = "workflow_executions"
        
        id = Column(Integer, primary_key=True, index=True)
        workflow_name = Column(String(100), nullable=False)
        steps_config = Column(JSON, nullable=False)
        current_step = Column(Integer, default=0)
        results = Column(JSON, nullable=True)
        status = Column(String(20), default="initialized")
        user_session = Column(String(100), nullable=True)
        created_at = Column(DateTime, default=datetime.utcnow)
        completed_at = Column(DateTime, nullable=True)
        total_tokens_used = Column(Integer, default=0)
        estimated_cost_usd = Column(String(20), nullable=True)

    class BrandTemplate(Base):
        __tablename__ = "brand_templates"
        
        id = Column(Integer, primary_key=True, index=True)
        name = Column(String(100), nullable=False)
        brand_colors = Column(JSON, nullable=True)
        typography = Column(JSON, nullable=True)
        voice_guidelines = Column(Text, nullable=True)
        logo_url = Column(String(500), nullable=True)
        style_rules = Column(JSON, nullable=True)
        created_at = Column(DateTime, default=datetime.utcnow)
        is_active = Column(Boolean, default=True)
else:
    print("‚ö†Ô∏è Database models disabled - SQLAlchemy not available")
    Base = None
    ContentGeneration = None
    WorkflowExecution = None
    BrandTemplate = None

# ==========================================================================
# PYDANTIC MODELS (REQUEST/RESPONSE SCHEMAS)
# ==========================================================================

if FASTAPI_AVAILABLE:
    class PromptStudioRequest(BaseModel):
        base_prompt: str = Field(..., min_length=1, max_length=2000, description="Az eredeti prompt sz√∂veg")
        platform: str = Field(..., description="C√©l platform: MidJourney, DALL-E, Runway, Stable-Diffusion")
        style_preferences: str = Field("realistic", description="St√≠lus preferenci√°k")
        aspect_ratio: Optional[str] = Field(None, pattern=r"^\d+:\d+$")
        quality_level: Optional[str] = Field("standard", pattern="^(draft|standard|premium)$")
        
        @field_validator('platform')
        @classmethod
        def validate_platform(cls, v):
            allowed = ['MidJourney', 'DALL-E', 'Runway', 'Stable-Diffusion']
            if v not in allowed:
                raise ValueError(f'Platform must be one of: {allowed}')
            return v

    class CaptionMasterRequest(BaseModel):
        content_description: str = Field(..., min_length=10, max_length=1000, description="Tartalom r√∂vid le√≠r√°sa")
        platform: str = Field(..., description="Social m√©dia platform")
        target_audience: str = Field("general", description="C√©lk√∂z√∂ns√©g le√≠r√°sa")  
        brand_voice: Optional[str] = Field("friendly", pattern="^(professional|casual|friendly|authoritative|playful)$")
        hashtag_count: int = Field(5, ge=0, le=30, description="Hashtag-ek sz√°ma")
        content_type: Optional[str] = Field("image", description="Tartalom t√≠pusa: image, video, carousel, story")
        cta_type: Optional[str] = Field(None, description="Call-to-action t√≠pusa: like, comment, share, visit, buy")
        keywords: Optional[List[str]] = Field(default_factory=list, description="Kulcsszavak list√°ja")
        
        @field_validator('platform')
        @classmethod
        def validate_platform(cls, v):
            allowed = ['Instagram', 'TikTok', 'LinkedIn', 'Twitter', 'YouTube', 'Facebook']
            if v not in allowed:
                raise ValueError(f'Platform must be one of: {allowed}')
            return v

    class VisualPrompterRequest(BaseModel):
        base_prompt: str = Field(..., min_length=5, max_length=500, description="Alapvet≈ë prompt le√≠r√°sa")
        composition_style: str = Field("balanced", pattern="^(close-up|medium-shot|wide-angle|balanced|rule-of-thirds)$")
        lighting: str = Field("natural", pattern="^(natural|studio|golden-hour|dramatic|soft|hard)$")
        color_palette: Optional[str] = Field(None, description="Sz√≠npaletta le√≠r√°s")
        mood: str = Field("neutral", pattern="^(energetic|calm|mysterious|cheerful|serious|neutral)$")
        technical_specs: Optional[str] = Field(None, description="Technikai specifik√°ci√≥k")
        output_format: str = Field("prompt", pattern="^(prompt|structured|parameters)$")

    class AudioScripterRequest(BaseModel):
        content_type: str = Field(..., pattern="^(podcast|youtube|voiceover|music-brief)$")
        topic: str = Field(..., min_length=5, max_length=200, description="T√©ma vagy c√≠m")
        duration_minutes: int = Field(5, ge=1, le=60, description="Id≈ëtartam percben")
        tone: str = Field("conversational", pattern="^(conversational|professional|educational|entertaining)$")
        target_audience: str = Field("general", description="C√©lk√∂z√∂ns√©g")
        
    class BrandTemplateRequest(BaseModel):
        brand_name: str = Field(..., min_length=2, max_length=50, description="M√°rka n√©v")
        industry: str = Field(..., min_length=3, max_length=50, description="Ipar√°g")
        target_audience: str = Field(..., min_length=5, max_length=200, description="C√©lk√∂z√∂ns√©g le√≠r√°sa")
        brand_values: str = Field(..., min_length=10, max_length=500, description="M√°rka √©rt√©kek")
        preferred_tone: str = Field("professional", description="Prefer√°lt hangnem")

    class WorkflowManagerRequest(BaseModel):
        workflow_name: str = Field(..., min_length=3, max_length=100, description="Workflow neve")
        steps: List[str] = Field(..., min_items=2, max_items=8, description="L√©p√©sek list√°ja")
        input_parameters: Optional[dict] = Field(None, description="Bemeneti param√©terek")
        output_format: str = Field("json", description="Kimeneti form√°tum")

    # Response modellek
    class GenerationResponse(BaseModel):
        success: bool = Field(..., description="Sikeres volt-e a gener√°l√°s")
        generation_id: str = Field(..., description="Egyedi gener√°l√°si azonos√≠t√≥")
        content: str = Field(..., description="Gener√°lt tartalom")
        metadata: Optional[dict] = Field(None, description="Tov√°bbi metaadatok")
        processing_time_ms: Optional[int] = Field(None, description="Feldolgoz√°si id≈ë milliszekundumban")
        suggestions: Optional[List[str]] = Field(None, description="Javasolt tov√°bbfejleszt√©sek")
        model_used: Optional[str] = Field(None, description="Haszn√°lt AI modell")
        token_usage: Optional[dict] = Field(None, description="Token felhaszn√°l√°s statisztik√°i")

    class WorkflowStatusResponse(BaseModel):
        workflow_id: str
        status: str
        current_step: int
        total_steps: int
        results: Optional[Dict[str, Any]] = None
        estimated_completion: Optional[datetime] = None
else:
    print("‚ö†Ô∏è Pydantic models disabled - FastAPI not available")
    PromptStudioRequest = None
    CaptionMasterRequest = None
    VisualPrompterRequest = None
    AudioScripterRequest = None
    BrandTemplateRequest = None
    WorkflowManagerRequest = None
    GenerationResponse = None
    WorkflowStatusResponse = None

# ==========================================================================
# OPENROUTER AI SERVICE MANAGER
# ===========================================================================

class AIServiceManager:
    """OpenRouter-alap√∫ AI szolg√°ltat√°skezel≈ë k√∂lts√©ghat√©kony modellekkel"""
    
    def __init__(self):
        # OpenRouter kliens inicializ√°l√°sa
        if OPENAI_AVAILABLE and OPENROUTER_API_KEY:
            self.client = AsyncOpenAI(
                api_key=OPENROUTER_API_KEY,
                base_url=OPENROUTER_BASE_URL
            )
            self.available = True
        else:
            self.client = None
            self.available = False
            print("‚ö†Ô∏è AI Service Manager disabled - Missing OpenAI client or API key")
        
        # Token haszn√°lat k√∂vet√©se
        self.daily_tokens_used = 0
        self.request_count = 0
        
        if self.available:
            logger.info(f"AIServiceManager initialized with OpenRouter")
            logger.info(f"Primary model: {MODEL_PRIMARY}")
            logger.info(f"Multimodal model: {MODEL_MULTIMODAL}")

    def initialize(self):
        """Szolg√°ltat√°s inicializ√°l√°sa √©s valid√°l√°sa"""
        try:
            if not self.available:
                logger.warning("AI Service Manager not available - dependencies missing")
                return False
            
            # OpenRouter kapcsolat tesztel√©se (opcion√°lis)
            logger.info("‚úÖ AI Service Manager inicializ√°lva")
            logger.info(f"   Primary model: {MODEL_PRIMARY}")
            logger.info(f"   Fallback model: {MODEL_FALLBACK}")
            logger.info(f"   Multimodal model: {MODEL_MULTIMODAL}")
            
            # Token l√≠mitekr≈ël t√°j√©koztat√°s
            logger.info(f"   Daily token limit: {DAILY_TOKEN_LIMIT}")
            logger.info(f"   Rate limit: {RATE_LIMIT_RPM} requests/minute")
            
            return True
            
        except Exception as e:
            logger.error(f"AI Service Manager initialization failed: {str(e)}")
            return False

    async def generate_content(self, 
                             prompt: str, 
                             model: Optional[str] = None,
                             max_tokens: int = DEFAULT_MAX_TOKENS,
                             temperature: float = DEFAULT_TEMPERATURE,
                             use_multimodal: bool = False) -> Dict[str, Any]:
        """
        Tartalom gener√°l√°s OpenRouter API-val
        """
        if not self.available:
            return {
                "success": False,
                "error": "AI Service not available - missing dependencies or API key",
                "content": f"Mock response for: {prompt[:100]}..."
            }
        
        # Rate limiting ellen≈ërz√©s
        if self.daily_tokens_used >= DAILY_TOKEN_LIMIT:
            if FASTAPI_AVAILABLE:
                raise HTTPException(
                    status_code=429, 
                    detail=f"Daily token limit ({DAILY_TOKEN_LIMIT}) exceeded"
                )
            else:
                return {"success": False, "error": "Daily token limit exceeded"}
        
        # Modell kiv√°laszt√°s
        if model is None:
            if use_multimodal:
                model = MODEL_MULTIMODAL
            else:
                model = MODEL_PRIMARY
        
        start_time = datetime.now()
        
        try:
            # OpenRouter API h√≠v√°s
            response = await self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature,
                timeout=DEFAULT_TIMEOUT,
                extra_headers={
                    "HTTP-Referer": SITE_URL,
                    "X-Title": APP_NAME
                }
            )
            
            # V√°lasz feldolgoz√°s
            content = response.choices[0].message.content
            usage = response.usage
            
            # Token haszn√°lat friss√≠t√©se
            input_tokens = usage.prompt_tokens if usage else 0
            output_tokens = usage.completion_tokens if usage else 0
            total_tokens = input_tokens + output_tokens
            
            self.daily_tokens_used += total_tokens
            self.request_count += 1
            
            # Id≈ëm√©r√©s
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            
            logger.info(f"Generated content with {model}: {input_tokens}+{output_tokens}={total_tokens} tokens")
            
            return {
                "success": True,
                "content": content,
                "model_used": model,
                "processing_time_ms": int(processing_time),
                "token_usage": {
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "total_tokens": total_tokens
                },
                "daily_usage": {
                    "tokens_used": self.daily_tokens_used,
                    "requests_made": self.request_count,
                    "limit_remaining": DAILY_TOKEN_LIMIT - self.daily_tokens_used
                }
            }
            
        except Exception as e:
            logger.error(f"Content generation failed with {model}: {str(e)}")
            
            # Fallback model haszn√°lata
            if model != MODEL_FALLBACK:
                logger.info(f"Retrying with fallback model: {MODEL_FALLBACK}")
                return await self.generate_content(
                    prompt=prompt,
                    model=MODEL_FALLBACK,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    use_multimodal=False
                )
            
            if FASTAPI_AVAILABLE:
                raise HTTPException(
                    status_code=500, 
                    detail=f"AI service error: {str(e)}"
                )
            else:
                return {"success": False, "error": f"AI service error: {str(e)}"}

    def get_usage_stats(self) -> Dict[str, Any]:
        """Napi haszn√°lati statisztik√°k"""
        return {
            "daily_tokens_used": self.daily_tokens_used,
            "daily_token_limit": DAILY_TOKEN_LIMIT,
            "usage_percentage": (self.daily_tokens_used / DAILY_TOKEN_LIMIT) * 100,
            "requests_made": self.request_count,
            "requests_per_minute_limit": RATE_LIMIT_RPM,
            "service_available": self.available
        }

# Global AI service instance
ai_service_manager = AIServiceManager()

# ==========================================================================
# BUSINESS LOGIC SERVICES
# ==========================================================================

class PromptStudioService:
    def __init__(self, ai_service_manager: AIServiceManager):
        self.ai_manager = ai_service_manager
        self.platform_templates = {
            "MidJourney": {
                "prefix": "",
                "suffix_template": " --ar {aspect_ratio} --v 6 --style raw",
                "optimization_prompt": """
                Optimaliz√°ld ezt a MidJourney promptot a legjobb vizu√°lis eredm√©ny√©rt.
                Fontos szempontok:
                - R√©szletes le√≠r√°s m≈±v√©szi st√≠lussal
                - Vil√°g√≠t√°s √©s kompoz√≠ci√≥ megad√°sa
                - Sz√≠npaletta √©s hangulat
                - Technikai param√©terek (--ar, --v, --style)
                
                Eredeti prompt: {base_prompt}
                St√≠lus m√≥dos√≠t√≥k: {style_modifiers}
                
                Add vissza:
                1. Optimaliz√°lt prompt
                2. 3 alternat√≠v vari√°ci√≥
                3. Technikai javaslatok
                4. V√°rhat√≥ eredm√©ny le√≠r√°sa
                """
            },
            "DALL-E": {
                "prefix": "",
                "suffix_template": "",
                "optimization_prompt": """
                Optimaliz√°ld ezt a DALL-E promptot a legjobb eredm√©ny√©rt.
                DALL-E specifikus szempontok:
                - Vil√°gos, konkr√©t le√≠r√°s
                - Fot√≥realisztikus vagy m≈±v√©szi st√≠lus megad√°sa
                - R√©szletes k√∂rnyezet √©s objektumok
                - Ker√ºld a t√∫l komplex kompoz√≠ci√≥kat
                
                Eredeti prompt: {base_prompt}
                St√≠lus m√≥dos√≠t√≥k: {style_modifiers}
                
                Add vissza:
                1. Optimaliz√°lt prompt
                2. 3 alternat√≠v vari√°ci√≥
                3. St√≠lus javaslatok
                4. V√°rhat√≥ eredm√©ny le√≠r√°sa
                """
            },
            "Runway": {
                "prefix": "",
                "suffix_template": "",
                "optimization_prompt": """
                Optimaliz√°ld ezt a Runway AI promptot vide√≥ gener√°l√°shoz.
                Runway specifikus szempontok:
                - Mozg√°s √©s akci√≥ le√≠r√°sa
                - Kamera mozg√°s megad√°sa
                - Id≈ëtartam √©s temp√≥
                - Vizu√°lis kontinuit√°s
                
                Eredeti prompt: {base_prompt}
                St√≠lus m√≥dos√≠t√≥k: {style_modifiers}
                
                Add vissza:
                1. Optimaliz√°lt prompt
                2. 3 alternat√≠v vari√°ci√≥
                3. Mozg√°s javaslatok
                4. V√°rhat√≥ eredm√©ny le√≠r√°sa
                """
            },
            "Stable-Diffusion": {
                "prefix": "",
                "suffix_template": ", highly detailed, 8k, photorealistic",
                "optimization_prompt": """
                Optimaliz√°ld ezt a Stable Diffusion promptot.
                SD specifikus szempontok:
                - Pozit√≠v √©s negat√≠v promptok
                - Kwalit√°s kulcsszavak haszn√°lata
                - M≈±v√©sz √©s st√≠lus hivatkoz√°sok
                - R√©szletes k√∂rnyezet le√≠r√°s
                
                Eredeti prompt: {base_prompt}
                St√≠lus m√≥dos√≠t√≥k: {style_modifiers}
                
                Add vissza:
                1. Optimaliz√°lt pozit√≠v prompt
                2. Aj√°nlott negat√≠v prompt
                3. 2 alternat√≠v vari√°ci√≥
                4. V√°rhat√≥ eredm√©ny le√≠r√°sa
                """
            }
        }
    
    async def generate_optimized_prompt(self, request: PromptStudioRequest) -> Dict[str, Any]:
        """Optimaliz√°lt prompt gener√°l√°s AI-val"""
        
        # Platform konfigur√°ci√≥ lek√©r√©se
        platform_config = self.platform_templates.get(request.platform)
        if not platform_config:
            raise HTTPException(status_code=400, detail=f"Unsupported platform: {request.platform}")
        
        # Optimaliz√°ci√≥s prompt √∂ssze√°ll√≠t√°sa
        optimization_prompt = platform_config["optimization_prompt"].format(
            base_prompt=request.base_prompt,
            style_modifiers=", ".join(request.style_modifiers) if request.style_modifiers else "nincs"
        )
        
        # Modell kiv√°laszt√°s min≈ës√©g alapj√°n
        model = MODEL_PRIMARY if request.quality_level == "premium" else MODEL_FALLBACK
        max_tokens = 1500 if request.quality_level == "premium" else 1000
        
        # AI h√≠v√°s az optimaliz√°ci√≥hoz
        result = await self.ai_manager.generate_content(
            prompt=optimization_prompt,
            model=model,
            max_tokens=max_tokens,
            temperature=0.6  # Kicsit alacsonyabb kreativit√°s a konzisztenci√°√©rt
        )
        
        if not result["success"]:
            raise HTTPException(status_code=500, detail="Failed to generate optimized prompt")
        
        # V√°lasz parseing (egyszer≈±s√≠tett)
        ai_response = result["content"]
        
        # Eredm√©ny struktur√°l√°sa
        try:
            # Egyszer≈± parseing - val√≥s implement√°ci√≥ban regex vagy LLM parseing kell
            lines = ai_response.strip().split('\n')
            optimized_prompt = ""
            alternatives = []
            technical_suggestions = ""
            expected_outcome = ""
            
            current_section = "optimized"
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                if "optimaliz√°lt prompt" in line.lower() or line.startswith("1."):
                    current_section = "optimized"
                elif "alternat√≠v" in line.lower() or line.startswith("2."):
                    current_section = "alternatives"
                elif "technikai" in line.lower() or "javaslat" in line.lower() or line.startswith("3."):
                    current_section = "technical"
                elif "v√°rhat√≥" in line.lower() or line.startswith("4."):
                    current_section = "outcome"
                else:
                    if current_section == "optimized" and not optimized_prompt:
                        optimized_prompt = line
                    elif current_section == "alternatives":
                        if line and not line.startswith(("2.", "3.", "4.")):
                            alternatives.append(line)
                    elif current_section == "technical":
                        technical_suggestions += line + " "
                    elif current_section == "outcome":
                        expected_outcome += line + " "
            
            # Suffix hozz√°ad√°sa ha sz√ºks√©ges
            if platform_config["suffix_template"] and request.aspect_ratio:
                suffix = platform_config["suffix_template"].format(
                    aspect_ratio=request.aspect_ratio
                )
                optimized_prompt += suffix
            
            await self._log_generation(request, ai_response, result)
            
            return {
                "optimized_prompt": optimized_prompt or ai_response[:200] + "...",
                "alternatives": alternatives[:3] if alternatives else [ai_response[:100]],
                "technical_suggestions": technical_suggestions.strip() or "Haszn√°ld az alap√©rtelmezett be√°ll√≠t√°sokat",
                "expected_outcome": expected_outcome.strip() or "Magas min≈ës√©g≈± vizu√°lis eredm√©ny v√°rhat√≥",
                "platform": request.platform,
                "quality_level": request.quality_level,
                "processing_info": {
                    "model_used": result["model_used"],
                    "processing_time_ms": result["processing_time_ms"],
                    "token_usage": result["token_usage"]
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to parse AI response: {str(e)}")
            # Fallback: return raw response
            return {
                "optimized_prompt": ai_response,
                "alternatives": [],
                "technical_suggestions": "Raw AI response due to parsing error",
                "expected_outcome": "Check the optimized_prompt field for full response",
                "platform": request.platform,
                "quality_level": request.quality_level,
                "processing_info": {
                    "model_used": result["model_used"],
                    "processing_time_ms": result["processing_time_ms"],
                    "token_usage": result["token_usage"]
                }
            }

    async def _log_generation(self, request: PromptStudioRequest, result: str, metadata: Dict[str, Any]):
        """Gener√°l√°s napl√≥z√°sa adatb√°zisba"""
        # Itt implement√°lni kell az adatb√°zis kapcsolatot
        logger.info(f"Generated prompt for {request.platform}: {len(result)} chars")

class CaptionMasterService:
    """Social media caption gener√°l√°si szolg√°ltat√°s"""
    
    def __init__(self, ai_service_manager: AIServiceManager):
        self.ai_manager = ai_service_manager
        
        # Platform-specifikus limitek √©s sabonok
        self.platform_configs = {
            "Instagram": {
                "max_length": 2200,
                "optimal_length": 150,
                "hashtag_optimal": 11,
                "style_guide": "vizu√°lis storytelling, lifestyle, authentic",
                "cta_examples": ["Dupla koppint√°s ‚ù§Ô∏è", "Mentsd el k√©s≈ëbb üîñ", "Mondd el kommentben üí¨"]
            },
            "LinkedIn": {
                "max_length": 3000,
                "optimal_length": 200,
                "hashtag_optimal": 3,
                "style_guide": "professional, insights, value-driven",
                "cta_examples": ["Mit gondolsz err≈ël?", "Oszd meg tapasztalatod", "Kapcsol√≥dj hozz√°m"]
            },
            "TikTok": {
                "max_length": 300,
                "optimal_length": 100,
                "hashtag_optimal": 5,
                "style_guide": "trendy, engaging, youth-oriented",
                "cta_examples": ["K√∂vetelj t√∂bb ilyen tartalom√©rt üî•", "Duplik√°lj ha egyet√©rtesz", "V√°laszolj vide√≥val"]
            },
            "YouTube": {
                "max_length": 5000,
                "optimal_length": 300,
                "hashtag_optimal": 15,
                "style_guide": "informative, community-building",
                "cta_examples": ["Iratkozz fel!", "Like ha tetszett", "Kommentelj k√©rd√©seket"]
            }
        }

    async def generate_platform_caption(self, request: CaptionMasterRequest, ai_manager: AIServiceManager) -> Dict[str, Any]:
        """Platform-specifikus caption gener√°l√°s"""
        
        # Platform konfigur√°ci√≥ lek√©r√©se
        platform_config = self.platform_configs.get(request.platform)
        if not platform_config:
            raise HTTPException(status_code=400, detail=f"Unsupported platform: {request.platform}")
        
        # Optimaliz√°ci√≥s prompt √∂ssze√°ll√≠t√°sa
        optimization_prompt = f"""
        Gener√°lj egy professzion√°lis {request.platform} captiot az al√°bbi param√©terek alapj√°n:
        
        PLATFORM: {request.platform}
        TARTALOM T√çPUS: {request.content_type}
        C√âLCSOPORT: {request.target_audience}
        BRAND HANGV√âTEL: {request.brand_voice}
        CALL-TO-ACTION: {request.cta_type or "engagement"}
        
        PLATFORM SPECIFIKUS IR√ÅNYELVEK:
        - Maxim√°lis hossz: {platform_config['max_length']} karakter
        - Optim√°lis hossz: {platform_config['optimal_length']} karakter
        - Aj√°nlott hashtag sz√°m: {platform_config['hashtag_optimal']}
        - St√≠lus √∫tmutat√≥: {platform_config['style_guide']}
        
        KULCSSZAVAK: {', '.join(request.keywords) if request.keywords else 'nincs megadva'}
        
        A caption tartalmazzon:
        1. Figyelemfelkelt≈ë nyit√≥ mondatot
        2. √ârt√©kes tartalmat a c√©lcsoportnak
        3. Megfelel≈ë emotikonokat (de ne t√∫lz√°sba)
        4. Hat√°sos call-to-action-t
        5. Relev√°ns hashtageket
        
        Add vissza:
        - F≈êCAPTION: a teljes caption sz√∂veg
        - ALTERNAT√çV√ÅK: 2 r√∂videbb/hosszabb vari√°ci√≥
        - HASHTAG_JAVASLATOK: platformra optimaliz√°lt hashtagek
        - ENGAGEMENT_TIPPEK: tov√°bbi engagement n√∂vel≈ë javaslatok
        """
        
        # Modell kiv√°laszt√°s
        model = MODEL_PRIMARY if request.brand_voice == "professional" else MODEL_FALLBACK
        max_tokens = 1200
        
        # AI h√≠v√°s
        result = await ai_manager.generate_content(
            prompt=optimization_prompt,
            model=model,
            max_tokens=max_tokens,
            temperature=0.8  # Kreat√≠vabb tartalom
        )
        
        if not result["success"]:
            raise HTTPException(status_code=500, detail="Failed to generate caption")
        
        # V√°lasz parseing
        ai_response = result["content"]
        
        try:
            lines = ai_response.strip().split('\n')
            main_caption = ""
            alternatives = []
            hashtag_suggestions = []
            engagement_tips = []
            
            current_section = "main"
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                if "f≈ëcaption" in line.lower() or "main" in line.lower():
                    current_section = "main"
                elif "alternat√≠v" in line.lower() or "alternative" in line.lower():
                    current_section = "alternatives"
                elif "hashtag" in line.lower():
                    current_section = "hashtags"
                elif "engagement" in line.lower() or "tipp" in line.lower():
                    current_section = "tips"
                else:
                    if current_section == "main" and not main_caption:
                        main_caption = line
                    elif current_section == "alternatives":
                        if line and not line.startswith(("ALTERNAT√çV", "HASHTAG", "ENGAGEMENT")):
                            alternatives.append(line)
                    elif current_section == "hashtags":
                        # Hashtag extraction
                        hashtags = [word for word in line.split() if word.startswith("#")]
                        hashtag_suggestions.extend(hashtags)
                    elif current_section == "tips":
                        engagement_tips.append(line)
            
            # Fallback ha nem siker√ºlt parseing
            if not main_caption:
                main_caption = ai_response[:platform_config['max_length']]
            
            # Hossz ellen≈ërz√©s √©s v√°g√°s
            if len(main_caption) > platform_config['max_length']:
                main_caption = main_caption[:platform_config['max_length']-3] + "..."
            
            # Hashtag limit alkalmaz√°sa
            if len(hashtag_suggestions) > request.hashtag_count:
                hashtag_suggestions = hashtag_suggestions[:request.hashtag_count]
            
            return {
                "caption": main_caption,
                "alternatives": alternatives[:2],
                "hashtags": hashtag_suggestions,
                "engagement_tips": engagement_tips[:3],
                "platform_stats": {
                    "character_count": len(main_caption),
                    "word_count": len(main_caption.split()),
                    "hashtag_count": len(hashtag_suggestions),
                    "platform_limit": platform_config['max_length'],
                    "optimal_length": platform_config['optimal_length']
                },
                "processing_info": {
                    "model_used": result["model_used"],
                    "processing_time_ms": result["processing_time_ms"],
                    "token_usage": result["token_usage"]
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to parse caption response: {str(e)}")
            # Fallback: return raw response
            return {
                "caption": ai_response[:platform_config['max_length']],
                "alternatives": [],
                "hashtags": [],
                "engagement_tips": ["Check the caption field for full AI response"],
                "platform_stats": {
                    "character_count": len(ai_response),
                    "word_count": len(ai_response.split()),
                    "hashtag_count": 0,
                    "platform_limit": platform_config['max_length'],
                    "optimal_length": platform_config['optimal_length']
                },
                "processing_info": {
                    "model_used": result["model_used"],
                    "processing_time_ms": result["processing_time_ms"],
                    "token_usage": result["token_usage"]
                }
            }

# ==========================================================================
# FASTAPI ALKALMAZ√ÅS √âS V√âGPONTOK
# ===========================================================================

# Global AI service instance  
ai_service_manager = AIServiceManager()

# Lifecycle manager
if FASTAPI_AVAILABLE:
    @asynccontextmanager
    async def lifespan(app: FastAPI):
        # Startup
        logger.info("üöÄ ContentHUB Backend inicializ√°l√°sa...")
        logger.info(f"OpenRouter integration: {OPENROUTER_API_KEY[:10] if OPENROUTER_API_KEY else 'Not configured'}...")
        logger.info(f"Primary AI model: {MODEL_PRIMARY}")
        logger.info(f"Multimodal model: {MODEL_MULTIMODAL}")
        yield
        # Shutdown
        logger.info("ContentHUB Backend API shutting down...")

    # FastAPI app inicializ√°l√°s
    app = FastAPI(
        title="ContentHUB Backend API v2.0",
        description="Multimod√°lis tartalom gener√°l√°s √©s prompt engineering platform",
        version="2.0.0",
        docs_url="/docs",
        redoc_url="/redoc",
        lifespan=lifespan
    )

    # Middleware be√°ll√≠t√°sok
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["http://localhost:3002", "http://localhost:8080", "*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"]
    )

    app.add_middleware(GZipMiddleware, minimum_size=1000)
else:
    print("‚ö†Ô∏è FastAPI application disabled - FastAPI not available")
    app = None

# ===========================================================================
# API V√âGPONTOK - CSAK HA FASTAPI EL√âRHET≈ê
# ===========================================================================

if FASTAPI_AVAILABLE and app:
    @app.get("/api/v1/health")
    async def health_check():
        """Rendszer health check √©s konfigur√°ci√≥"""
        usage_stats = ai_service_manager.get_usage_stats()
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "version": "2.0.0",
            "ai_config": {
                "primary_model": MODEL_PRIMARY,
                "fallback_model": MODEL_FALLBACK,
                "multimodal_model": MODEL_MULTIMODAL,
                "openrouter_connected": bool(OPENROUTER_API_KEY)
            },
            "usage_stats": usage_stats,
            "features": {
                "prompt_studio": True,
                "caption_master": True,
                "visual_prompter": True,
                "audio_scripter": True,
                "brand_templater": True,
                "workflow_manager": True
            }
        }

    @app.get("/api/v1/usage-stats")
    async def get_usage_statistics():
        """Napi haszn√°lati statisztik√°k lek√©rdez√©se"""
        return ai_service_manager.get_usage_stats()

    # ===========================================================================
    # PROMPT STUDIO ENDPOINTS
    # ===========================================================================

    @app.post("/api/v1/prompt-studio/generate", response_model=GenerationResponse)
    async def generate_optimized_prompt(request: PromptStudioRequest):
        """Optimaliz√°lt prompt gener√°l√°s AI-val"""
        try:
            prompt_service = PromptStudioService(ai_service_manager)
            result = await prompt_service.generate_optimized_prompt(request)
            
            return GenerationResponse(
                success=True,
                generation_id=f"ps_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                content=result["optimized_prompt"],
                metadata={
                    "alternatives": result["alternatives"],
                    "technical_suggestions": result["technical_suggestions"],
                    "expected_outcome": result["expected_outcome"],
                    "platform": result["platform"]
                },
                processing_time_ms=result["processing_info"]["processing_time_ms"],
                suggestions=result["alternatives"],
                model_used=result["processing_info"]["model_used"],
                token_usage=result["processing_info"]["token_usage"]
            )
            
        except Exception as e:
            logger.error(f"Prompt generation failed: {str(e)}")
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/api/v1/prompt-studio/templates")
    async def get_prompt_templates():
        """El≈ëre defini√°lt prompt sablonok lek√©rdez√©se"""
        templates = {
            "portrait_photography": {
                "name": "Portr√© Fot√≥z√°s",
                "base_prompt": "Professional portrait of a person, studio lighting, shallow depth of field",
                "modifiers": ["cinematic", "high resolution", "professional"],
                "recommended_aspect_ratio": "3:4",
                "platforms": ["MidJourney", "DALL-E", "Stable-Diffusion"]
            },
            "landscape_art": {
                "name": "T√°jk√©p M≈±v√©szet",
                "base_prompt": "Breathtaking landscape view, natural lighting, panoramic composition",
                "modifiers": ["scenic", "nature", "atmospheric"],
                "recommended_aspect_ratio": "16:9",
                "platforms": ["MidJourney", "DALL-E", "Runway"]
            },
            "product_photography": {
                "name": "Term√©k Fot√≥z√°s",
                "base_prompt": "Clean product shot, white background, professional lighting",
                "modifiers": ["commercial", "clean", "minimalist"],
                "recommended_aspect_ratio": "1:1",
                "platforms": ["DALL-E", "Stable-Diffusion"]
            },
            "architectural_design": {
                "name": "√âp√≠t√©szeti Design",
                "base_prompt": "Modern architectural structure, clean lines, innovative design",
                "modifiers": ["contemporary", "geometric", "structural"],
                "recommended_aspect_ratio": "16:9",
                "platforms": ["MidJourney", "DALL-E"]
            },
            "character_concept": {
                "name": "Karakter Koncepci√≥",
                "base_prompt": "Character design concept art, detailed features, expressive",
                "modifiers": ["concept art", "detailed", "creative"],
                "recommended_aspect_ratio": "3:4",
                "platforms": ["MidJourney", "Stable-Diffusion"]
            },
            "video_scene": {
                "name": "Vide√≥ Jelenet",
                "base_prompt": "Cinematic scene with dynamic camera movement and lighting",
                "modifiers": ["cinematic", "dynamic", "atmospheric"],
                "recommended_aspect_ratio": "16:9",
                "platforms": ["Runway"]
            }
        }
        
        return {
            "success": True,
            "templates": templates,
            "total_count": len(templates),
            "categories": ["photography", "art", "design", "video"]
        }

    # ===========================================================================
    # CAPTION MASTER ENDPOINTS
    # ===========================================================================

    @app.post("/api/v1/caption-master/generate", response_model=GenerationResponse)
    async def generate_social_caption(request: CaptionMasterRequest):
        """Platform-optimaliz√°lt social media caption gener√°l√°s"""
        try:
            caption_service = CaptionMasterService(ai_service_manager)
            result = await caption_service.generate_platform_caption(request, ai_service_manager)
            
            return GenerationResponse(
                success=True,
                generation_id=f"cm_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                content=result["caption"],
                metadata={
                    "alternatives": result["alternatives"],
                    "hashtags": result["hashtags"],
                    "engagement_tips": result["engagement_tips"],
                    "platform_stats": result["platform_stats"]
                },
                processing_time_ms=result["processing_info"]["processing_time_ms"],
                suggestions=result["engagement_tips"],
                model_used=result["processing_info"]["model_used"],
                token_usage=result["processing_info"]["token_usage"]
            )
            
        except Exception as e:
            logger.error(f"Caption generation failed: {str(e)}")
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/api/v1/caption-master/platform-info/{platform}")
    async def get_platform_information(platform: str):
        """Platform-specifikus inform√°ci√≥k √©s limitek"""
        caption_service = CaptionMasterService(ai_service_manager)
        
        if platform not in caption_service.platform_configs:
            raise HTTPException(status_code=404, detail=f"Platform {platform} not supported")
        
        config = caption_service.platform_configs[platform]
        
        return {
            "platform": platform,
            "configuration": config,
            "best_practices": {
                "post_timing": f"Optim√°lis id≈ëpontok {platform}-hoz",
                "content_types": f"Legjobb tartalom t√≠pusok {platform}-hoz",
                "engagement_strategies": f"Engagement strat√©gi√°k {platform}-hoz"
            },
            "supported_features": {
                "hashtags": True,
                "mentions": True,
                "emojis": True,
                "links": platform in ["LinkedIn", "YouTube"]
            }
        }

    # ===========================================================================
    # ADDITIONAL MODULES (PLACEHOLDER ENDPOINTS)
    # ===========================================================================

    @app.post("/api/v1/visual-prompter/analyze")
    async def analyze_visual_content(request: VisualPrompterRequest):
        """Vizu√°lis tartalom elemz√©s √©s prompt gener√°l√°s"""
        return {
            "message": "Visual Prompter module - coming soon",
            "request_received": request.dict(),
            "estimated_completion": "Q2 2025"
        }

    @app.post("/api/v1/audio-scripter/generate")
    async def generate_audio_script(request: AudioScripterRequest):
        """Audio script gener√°l√°s podcast/voiceover-hez"""
        return {
            "message": "Audio Scripter module - coming soon", 
            "request_received": request.dict(),
            "estimated_completion": "Q2 2025"
        }

    @app.post("/api/v1/brand-templater/create")
    async def create_brand_template(request: BrandTemplateRequest):
        """Brand template l√©trehoz√°s √©s t√°rol√°s"""
        return {
            "message": "Brand Templater module - coming soon",
            "request_received": request.dict(),
            "estimated_completion": "Q1 2025"
        }

    @app.post("/api/v1/workflow-manager/execute")
    async def execute_workflow(request: WorkflowManagerRequest):
        """Multi-step workflow futtat√°s"""
        return {
            "message": "Workflow Manager module - coming soon",
            "request_received": request.dict(),
            "estimated_completion": "Q2 2025"
        }

    # ===========================================================================
    # ERROR HANDLERS
    # ===========================================================================

    @app.exception_handler(HTTPException)
    async def http_exception_handler(request, exc):
        return JSONResponse(
            status_code=exc.status_code,
            content={
                "error": exc.detail,
                "status_code": exc.status_code,
                "timestamp": datetime.now().isoformat(),
                "path": str(request.url)
            }
        )

    @app.exception_handler(Exception)
    async def general_exception_handler(request, exc):
        logger.error(f"Unhandled exception: {str(exc)}")
        return JSONResponse(
            status_code=500,
            content={
                "error": "Internal server error",
                "message": "An unexpected error occurred",
                "timestamp": datetime.now().isoformat(),
                "path": str(request.url)
            }
        )
else:
    print("‚ö†Ô∏è API endpoints disabled - FastAPI not available")

# ===========================================================================
# FELEOLG√ì INICIALIZ√ÅL√ÅS √âS FUTTAT√ÅS
# ===========================================================================

def initialize_services():
    """Szolg√°ltat√°sok inicializ√°l√°sa √©s valid√°l√°sa"""
    try:
        logger.info("üöÄ ContentHUB Backend inicializ√°l√°sa...")
        
        # Environment config ellen≈ërz√©s
        if not OPENROUTER_API_KEY:
            logger.warning("‚ö†Ô∏è OPENROUTER_API_KEY nincs be√°ll√≠tva - korl√°tozott funkcionalit√°s")
        
        # AI Service Manager inicializ√°l√°s
        ai_service_manager.initialize()
        logger.info("‚úÖ AI Service Manager inicializ√°lva")
        
        # Database kapcsolat (ha el√©rhet≈ë)
        if DATABASE_URL and SQLALCHEMY_AVAILABLE:
            logger.info("üìä Database kapcsolat el√©rhet≈ë")
        else:
            logger.info("üìä Database m≈±k√∂d√©s opcion√°lis m√≥dban")
        
        logger.info("üéØ ContentHUB Backend sikeresen inicializ√°lva!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Inicializ√°l√°si hiba: {str(e)}")
        return False

def get_app():
    """FastAPI app getter funkci√≥k√©nt haszn√°lhat√≥"""
    if FASTAPI_AVAILABLE and app:
        return app
    else:
        logger.warning("FastAPI nem el√©rhet≈ë - backend API inakt√≠v")
        return None

if __name__ == "__main__":
    print("=" * 70)
    print("üé® CONTENTHUB BACKEND API v2.0")
    print("   Premium Multimod√°lis Tartalom Gener√°tor")
    print("   Aevorex Premium Team | 2025")
    print("=" * 70)
    
    # Inicializ√°l√°s
    init_success = initialize_services()
    
    if FASTAPI_AVAILABLE and init_success:
        try:
            import uvicorn
            
            print("\nüöÄ Szerver ind√≠t√°sa:")
            print(f"   URL: http://localhost:8085")
            print(f"   Dokument√°ci√≥: http://localhost:8085/docs")
            print(f"   Health check: http://localhost:8085/api/v1/health")
            print(f"   Modell: {MODEL_PRIMARY}")
            print("\nüí° El√©rhet≈ë szolg√°ltat√°sok:")
            print("   üìù Prompt Studio - AI prompt optimaliz√°l√°s")
            print("   üì± Caption Master - Social media caption gener√°l√°s")
            print("   üé• Visual Prompter - Vizu√°lis tartalom elemz√©s")
            print("   üéµ Audio Scripter - Audio script k√©sz√≠t√©s")
            print("   üè¢ Brand Templater - M√°rka sablon menedzser")
            print("   ‚öôÔ∏è Workflow Manager - Munkafolyamat automatiz√°l√°s")
            print("\n" + "=" * 70)
            
            # Uvicorn szerver ind√≠t√°sa
            uvicorn.run(
                app,
                host="0.0.0.0",
                port=8085,
                reload=False,
                access_log=True,
                log_level="info"
            )
            
        except ImportError:
            print("‚ùå Uvicorn nem el√©rhet≈ë - manu√°lis FastAPI futtat√°s sz√ºks√©ges")
            print("   Telep√≠t√©s: pip install uvicorn")
            print(f"   Futtat√°s: uvicorn main:app --host 0.0.0.0 --port 8085")
            
        except Exception as e:
            logger.error(f"Szerver ind√≠t√°si hiba: {str(e)}")
            
    else:
        print("\n‚ö†Ô∏è Backend API korl√°tozott m√≥dban fut")
        print("   Hi√°nyz√≥ f√ºgg≈ës√©gek: pip install -r requirements.txt")
        print("   Vagy: pip install fastapi uvicorn sqlalchemy pydantic")
        
        # Alapvet≈ë funkci√≥k tesztel√©se f√ºgg≈ës√©gek n√©lk√ºl is
        if init_success:
            print("\nüß™ Alapfunkci√≥k tesztel√©se...")
            try:
                test_stats = ai_service_manager.get_usage_stats()
                print(f"   ‚úÖ AI Service Manager m≈±k√∂dik: {test_stats}")
            except Exception as e:
                print(f"   ‚ùå Tesztol√°s sikertelen: {str(e)}")
    
    print("\nüèÅ ContentHUB Backend bet√∂ltve.")

# ===========================================================================
# EXPORT F≈êBB OBJEKTUMOK (IMPORT HASZN√ÅLATRA)
# ===========================================================================

__all__ = [
    "app",
    "ai_service_manager", 
    "initialize_services",
    "get_app"
] 